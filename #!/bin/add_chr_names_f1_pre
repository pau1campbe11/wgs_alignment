#!/bin/bash -l

############# SLURM SETTINGS #############
#SBATCH --account=project0005         # account name (mandatory), if the job runs under a project then it'll be the project name, if not then it should =none
#SBATCH --job-name=add_chr_name  # some descriptive job name of your choice
#SBATCH --output=%x-%J.out            # output file name will contain job name  + job ID
#SBATCH --error=%x-%J.err             # error file name will contain job name  + job ID
#SBATCH --time=0-01:00:00             # time limit for the whole run, in the form of d-hh:mm:ss, also accepts mm, mm:ss, hh:mm:ss, d-hh, d-hh:mm
#SBATCH --mem=1G                      # memory required per node, in the form of [num][M|G|T]
#SBATCH --ntasks=1                    # number of Slurm tasks to be launched, increase for multi-process runs ex. MPI
#SBATCH --cpus-per-task=10             # number of processor cores to be assigned for each task, default is 1, increase for multi-threaded runs
#SBATCH --ntasks-per-node=1           # number of tasks to be launched on each allocated node

### Code section


awk -v newcol="OZ172662.1" '{ print newcol, $0 }' f1pre_chr1_count.txt > complete_f2_chr1_pre_output.txt

awk -v newcol="OZ172663.1" '{ print newcol, $0 }' f1pre_chr2_count.txt > complete_f2_chr2_pre_output.txt

awk -v newcol="OZ172664.1" '{ print newcol, $0 }' f1pre_chr3_count.txt > complete_f2chr3_pre_output.txt

awk -v newcol="OZ172665.1" '{ print newcol, $0 }' f1pre_chr4_count.txt > complete_f2_chr4_pre_output.txt

awk -v newcol="OZ172666.1" '{ print newcol, $0 }' f1pre_chr5_count.txt > complete_f2_chr5_pre_output.txt

awk -v newcol="OZ172667.1" '{ print newcol, $0 }' f1pre_chrx_count.txt > complete_f2_chrx_pre_output.txt
